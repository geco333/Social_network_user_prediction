{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = ['./har_fit_0', './har_fit_1', './har_fit_2', './har_fit_3']\n",
    "fp = [FingerPrint(Har.from_csv(user), types=True) for user in users]\n",
    "ua = UsersAnalyzer(fp, flags='t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score each cross validation\n",
    "<p>5 Runs</p>\n",
    "<p>Fit 60%\n",
    "<p>Test 40%</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.52727273 0.47272727 0.45283019 0.53846154 0.51923077]\n"
     ]
    }
   ],
   "source": [
    "print(ua.score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.27      0.29        51\n",
      "           1       0.33      0.02      0.04        42\n",
      "           2       0.24      0.28      0.26        40\n",
      "           3       0.27      0.49      0.34        45\n",
      "\n",
      "   micro avg       0.27      0.27      0.27       178\n",
      "   macro avg       0.29      0.27      0.23       178\n",
      "weighted avg       0.29      0.27      0.24       178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(ua.y_fit_true[:len(ua.predictions)], ua.predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14  1 14 22]\n",
      " [12  1 12 17]\n",
      " [ 6  1 11 22]\n",
      " [14  0  9 22]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(ua.y_fit_true[:len(ua.predictions)], ua.predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, re, os, random, logging, time, traceback\n",
    "from functools import reduce\n",
    "import pandas as pd\n",
    "from scipy.stats import binom\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import selenium\n",
    "from seleniumwire import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from browsermobproxy import Server\n",
    "from collections import Counter\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "\n",
    "class Har:\n",
    "    @staticmethod\n",
    "    def from_csv(path: str) -> list:\n",
    "        \"\"\"Transform each har_fit file in the path to a Pandas DataFrame,\n",
    "            add each DataFrame to a list,\n",
    "            and return the list.\n",
    "\n",
    "        :param str path: The path to the har_fit files directory.\n",
    "        :return list: A list of the har_fit files data in Pandas DataFrames.\n",
    "        \"\"\"\n",
    "\n",
    "        hars = []\n",
    "\n",
    "        for file in os.listdir(path):\n",
    "            if file[-4:] == '.csv':\n",
    "                hars.append(pd.read_csv(f'{path}/{file}'))\n",
    "\n",
    "        return hars\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def capture_n_har_files(path: str, page: str = None, n: int = 1, name: str = '', url: str = '', rnd: bool = False,\n",
    "                            page_func=''):\n",
    "        \"\"\"Run n times:\n",
    "            Create an Har class instance.\n",
    "            Using BrowsermobProxy start recording har data from the browser.\n",
    "            Using selenium start a browser and go to the url.\n",
    "            Perform the action ordered by the 'page_func' function.\n",
    "            Create a Pandas DataFrame from the har data recorded.\n",
    "            Close the selenium session and the proxy.\n",
    "            Export the DataFrame to a csv file.\n",
    "\n",
    "        :param page_func: A custom function for action to perform inside the webpage.\n",
    "        :param path: csv file/s save location.\n",
    "        :param rnd: Whether to choose a random url.\n",
    "        :param n: Number of times to run.\n",
    "        :param name: Record name.\n",
    "        :param url: The web site, use full address(exm: http://www.google.com).\n",
    "        \"\"\"\n",
    "\n",
    "        urls = ['https://www.tumblr.com/', 'https://findtheinvisiblecow.com/', 'https://theuselessweb.com/',\n",
    "                'https://www.linkedin.com/', 'https://www.reddit.com/', 'https://www.taringa.net/',\n",
    "                'https://the-dots.com/', 'https://www.youtube.com/',\n",
    "                'https://www.reverbnation.com/', 'https://www.flixster.com/', 'https://www.care2.com/',\n",
    "                'https://www.ravelry.com/account/login'\n",
    "                'http://hackertyper.com/', 'https://www.instagram.com/', 'https://twitter.com/',\n",
    "                'https://www.pinterest.com/']\n",
    "\n",
    "        try:\n",
    "            _ = re.findall('\\\\d+', os.listdir(f'{path}').__str__())\n",
    "            __ = list(map(lambda x: int(x), _))\n",
    "            last_i = max(__) + 1\n",
    "        except ValueError:\n",
    "            last_i = 0\n",
    "\n",
    "        for i in range(last_i, last_i + n):\n",
    "            if rnd:\n",
    "                url = urls[random.randrange(len(urls))]\n",
    "\n",
    "            print(str(i) + ' ' + url)\n",
    "\n",
    "            har = Har()\n",
    "            har._capture_data(name, url, page_func=page_func, page=page)\n",
    "            har._build_df()\n",
    "            har.quit()\n",
    "\n",
    "            har.export_df(f'{path}/har_df_{i}.csv')\n",
    "\n",
    "\n",
    "    def _capture_data(self, name, url, page_func, page):\n",
    "        \"\"\"\n",
    "        :param name: The har output name.\n",
    "        :param url: The website to capture har data from.\n",
    "        :param page_func: A custom function ordering the actions to perform inside the web page.\n",
    "        \"\"\"\n",
    "\n",
    "        self.proxy.new_har(name)\n",
    "        self.driver.get(url)\n",
    "\n",
    "        if page_func != '':\n",
    "            page_func(self.driver, page)\n",
    "\n",
    "\n",
    "    def __init__(self, path=None):\n",
    "        if path is None:\n",
    "            self.server = Server(\n",
    "                'C:/Users/Geco/AngularProjects/BuildUrlDatabase/py/browsermob-proxy-2.1.4/bin/browsermob-proxy.bat')\n",
    "            self.server.start()\n",
    "            self.proxy = self._start_proxy()\n",
    "            self.driver = self._start_chrome_driver()\n",
    "            self.df = None\n",
    "        else:\n",
    "            self.df = pd.read_csv(f'{path}')\n",
    "\n",
    "\n",
    "    def _start_proxy(self):\n",
    "        \"\"\"Start a new proxy server to capture har data.\n",
    "\n",
    "        :return: The new server proxy.\n",
    "        \"\"\"\n",
    "\n",
    "        run = True\n",
    "\n",
    "        while run:\n",
    "            try:\n",
    "                proxy = self.server.create_proxy()\n",
    "                run = False\n",
    "            except requests.exceptions.ConnectionError as e:\n",
    "                print(e)\n",
    "\n",
    "        return proxy\n",
    "\n",
    "\n",
    "    def _start_chrome_driver(self) -> webdriver:\n",
    "        \"\"\"Using Selenium start the google chrome browser headless.\n",
    "        All the browser requests and responses(har_fit data) will be recorded\n",
    "        using a BrowsermobProxy proxy server.\n",
    "\n",
    "        :return: Google chrome driver object.\n",
    "        \"\"\"\n",
    "\n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        prefs = {\"profile.default_content_setting_values.notifications\": 2}\n",
    "        chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "        chrome_options.set_capability('proxy', {'httpProxy': f'{self.proxy.proxy}',\n",
    "                                                'noProxy': '',\n",
    "                                                'proxyType': 'manual',\n",
    "                                                'sslProxy': f'{self.proxy.proxy}'})\n",
    "        # chrome_options.add_argument(\"--headless\")\n",
    "\n",
    "        driver = webdriver.Chrome(chrome_options=chrome_options)\n",
    "        driver.set_page_load_timeout(999)\n",
    "        driver.delete_all_cookies()\n",
    "\n",
    "        return driver\n",
    "\n",
    "\n",
    "    def quit(self):\n",
    "        \"\"\"Close all open connections: Close the proxy server and the chrome driver.\n",
    "        \"\"\"\n",
    "        self.driver.quit()\n",
    "        self.server.stop()\n",
    "        os.system(\"taskkill /f /im java.exe\")\n",
    "\n",
    "\n",
    "    def export_har(self):\n",
    "        \"\"\"\n",
    "        Export the har_fit recording to a json file.\n",
    "        \"\"\"\n",
    "        with open('./har_fit.json', 'w') as file:\n",
    "            json.dump(self.proxy.har, file)\n",
    "\n",
    "\n",
    "    def export_df(self, path):\n",
    "        \"\"\"\n",
    "        Export the instance DataFrame to a csv file.\n",
    "        :param path: Export directory path.\n",
    "        \"\"\"\n",
    "        self.df.to_csv(path)\n",
    "\n",
    "\n",
    "    def _add_to_dict(self, __, k, v):\n",
    "        \"\"\"Utility method for the build_df method.\n",
    "        \"\"\"\n",
    "        if type(v) == list:\n",
    "            for kk, vv in v:\n",
    "                if type(vv) == dict or type(vv) == list:\n",
    "                    self._add_to_dict(__, k + kk + '.', vv)\n",
    "                else:\n",
    "                    __[k + kk] = vv\n",
    "        else:\n",
    "            for kk, vv in v.items():\n",
    "                if type(vv) == dict or type(vv) == list:\n",
    "                    self._add_to_dict(__, k + kk + '.', vv)\n",
    "                else:\n",
    "                    __[k + kk] = vv\n",
    "\n",
    "\n",
    "    def _build_df(self):\n",
    "        \"\"\"\n",
    "        Iterate each row in the har_fit data csv file\n",
    "        and add it to a dictionary.\n",
    "        Add all the rows dictionaries to a list.\n",
    "        Create one complete DataFrame from the list.\n",
    "\n",
    "        :return: The instance har_fit recording data in the form of a Pandas DataFrame.\n",
    "        \"\"\"\n",
    "        _ = list()\n",
    "\n",
    "        for entry in self.proxy.har['log']['entries']:\n",
    "            __ = dict()\n",
    "\n",
    "            for k, v in entry.items():\n",
    "                if type(v) == dict or type(v) == list:\n",
    "                    self._add_to_dict(__, k + '.', v)\n",
    "                else:\n",
    "                    __[k] = v\n",
    "\n",
    "            _.append(__)\n",
    "\n",
    "        self.df = pd.DataFrame(_)\n",
    "\n",
    "\n",
    "class FingerPrint:\n",
    "    def __init__(self, hars, types: bool = False):\n",
    "        self.hars = hars\n",
    "        self.length = len(hars)\n",
    "        self.sums = []\n",
    "        self.sessions = []\n",
    "        self.weights = None\n",
    "\n",
    "        self._init_data()\n",
    "        self._init_weights()\n",
    "\n",
    "        if types:\n",
    "            self.types_counts = self._gather_types()\n",
    "            self.types = self._get_types()\n",
    "\n",
    "\n",
    "    def _gather_types(self):\n",
    "        _ = []\n",
    "\n",
    "        for df in self.hars:\n",
    "            __ = {}\n",
    "\n",
    "            for row in df.iterrows():\n",
    "                try:\n",
    "                    ___ = re.findall('(?<=\\.)\\w{1,4}$', row[1]['request.url'])[0]\n",
    "\n",
    "                    if ___ in __:\n",
    "                        __[___] += 1\n",
    "                    else:\n",
    "                        __[___] = 1\n",
    "                except IndexError:\n",
    "                    pass\n",
    "\n",
    "            _.append(__)\n",
    "\n",
    "        return _\n",
    "\n",
    "\n",
    "    def _get_types(self):\n",
    "        _ = []\n",
    "\n",
    "        for session in self.types_counts:\n",
    "            for k in session.keys():\n",
    "                if k not in _:\n",
    "                    _.append(k)\n",
    "\n",
    "        return _\n",
    "\n",
    "\n",
    "    def _init_data(self):\n",
    "        for har in self.hars:\n",
    "            session_sums = [0, 0]\n",
    "            session = []\n",
    "\n",
    "            for row in har[['response.bodySize', 'response.headersSize', 'time']].values:\n",
    "                row = tuple(row.tolist())\n",
    "\n",
    "                # Add to sums.\n",
    "                session_sums[0] += row[0]\n",
    "                session_sums[1] += row[1]\n",
    "\n",
    "                session.append(row)\n",
    "\n",
    "            # Add to count.\n",
    "            self.sessions.append(session)\n",
    "            self.sums.append(session_sums)\n",
    "\n",
    "\n",
    "    def _flatten_sessions(self):\n",
    "        return [row for session in self.sessions for row in session]\n",
    "\n",
    "\n",
    "    def _init_weights(self):\n",
    "        flat_sessions = self._flatten_sessions()\n",
    "        self.weights = Counter(flat_sessions)\n",
    "\n",
    "\n",
    "class ResponseData:\n",
    "    def __init__(self, hars: list, types: bool = False):\n",
    "        self.hars = hars\n",
    "        self.length = len(hars)\n",
    "        self.sums = []\n",
    "        self.sessions = []\n",
    "\n",
    "        self._init_data()\n",
    "\n",
    "        if types:\n",
    "            self.types_counts = self._gather_types()\n",
    "\n",
    "\n",
    "    def _init_data(self):\n",
    "        \"\"\"Sum body size and header size individually for each session: (body size, header size)\n",
    "           Create a tuple of (body size, header size) per session then add each session to self.sessions.\"\"\"\n",
    "        for har in self.hars:\n",
    "            session_sums = [0, 0]\n",
    "            session = []\n",
    "\n",
    "            for row in har[['response.bodySize', 'response.headersSize', 'time']].values:\n",
    "                row = tuple(row.tolist())\n",
    "\n",
    "                session_sums[0] += row[0]\n",
    "                session_sums[1] += row[1]\n",
    "\n",
    "                session.append(row)\n",
    "\n",
    "            self.sums.append(session_sums)\n",
    "            self.sessions.append(session)\n",
    "\n",
    "\n",
    "    def _gather_types(self):\n",
    "        _ = []\n",
    "\n",
    "        for df in self.hars:\n",
    "            __ = {}\n",
    "\n",
    "            for row in df.iterrows():\n",
    "                try:\n",
    "                    ___ = re.findall('(?<=\\.)\\w{1,4}$', row[1]['request.url'])[0]\n",
    "\n",
    "                    if ___ in __:\n",
    "                        __[___] += 1\n",
    "                    else:\n",
    "                        __[___] = 1\n",
    "                except IndexError:\n",
    "                    pass\n",
    "\n",
    "            _.append(__)\n",
    "\n",
    "        return _\n",
    "\n",
    "\n",
    "class Analyzer:\n",
    "    def __init__(self, fp: FingerPrint, rd: ResponseData):\n",
    "        self.fp = fp  # Fit data.\n",
    "        self.rd = rd  # Predict data.\n",
    "        self.x_fit = []\n",
    "        self.x_predict = []\n",
    "        self.y_fit_true = []\n",
    "        self.y_predict_true = []\n",
    "\n",
    "\n",
    "    def plot_confusion_matrix(self, title=None, cmap=plt.cm.Blues):\n",
    "        \"\"\"\n",
    "        This function prints and plots the confusion matrix.\n",
    "        Normalization can be applied by setting `normalize=True`.\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute confusion matrix\n",
    "        cm = confusion_matrix(self.y_predict_true, self.predictions)\n",
    "        # Only use the labels that appear in the data\n",
    "        classes = unique_labels(self.y_fit_true, self.y_predict_true)\n",
    "\n",
    "        print(classification_report(self.y_predict_true, self.predictions, labels=[0, 1],\n",
    "                                    target_names=['facebook', 'other']))\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "        ax.figure.colorbar(im, ax=ax)\n",
    "        # We want to show all ticks...\n",
    "        ax.set(xticks=np.arange(cm.shape[1]),\n",
    "               yticks=np.arange(cm.shape[0]),\n",
    "               # ... and label them with the respective list entries\n",
    "               xticklabels=classes, yticklabels=classes,\n",
    "               title=title,\n",
    "               ylabel='True label',\n",
    "               xlabel='Predicted label')\n",
    "\n",
    "        # Rotate the tick labels and set their alignment.\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "                 rotation_mode=\"anchor\")\n",
    "\n",
    "        # Loop over data dimensions and create text annotations.\n",
    "        thresh = cm.max() / 2.\n",
    "\n",
    "        for i in range(cm.shape[0]):\n",
    "            for j in range(cm.shape[1]):\n",
    "                ax.text(j, i, format(cm[i, j], 'd'),\n",
    "                        ha=\"center\", va=\"center\",\n",
    "                        color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "        fig.tight_layout()\n",
    "\n",
    "        return ax\n",
    "\n",
    "\n",
    "class TypesAnalyzer(Analyzer):\n",
    "    def __init__(self, fp: FingerPrint, rd: ResponseData):\n",
    "        super().__init__(fp, rd)\n",
    "\n",
    "        self.dict_vectorizer = DictVectorizer(sparse=False)\n",
    "        self._init_data()\n",
    "        self.clf = self._classify()\n",
    "        self.predictions = self.clf.predict(self.x_predict)\n",
    "\n",
    "\n",
    "    def _init_data(self):\n",
    "        _ = []\n",
    "\n",
    "        for session_types in self.rd.types_counts:\n",
    "            __ = {}\n",
    "\n",
    "            for t in self.fp.types:\n",
    "                if t in session_types:\n",
    "                    __[t] = session_types[t]\n",
    "                else:\n",
    "                    __[t] = 0\n",
    "\n",
    "            _.append(__)\n",
    "\n",
    "        self.x_fit = self.dict_vectorizer.fit_transform(self.fp.types_counts)\n",
    "        self.y_fit_true = [1] * self.x_fit.shape[0]\n",
    "        self.x_predict = self.dict_vectorizer.transform(_)\n",
    "        self.y_predict_true = [0] * self.x_predict.shape[0]\n",
    "\n",
    "\n",
    "    def _classify(self):\n",
    "        self.x_fit, self.x_predict, self.y_fit_true, self.y_predict_true = train_test_split(\n",
    "            np.concatenate([self.x_fit.data, self.x_predict.data]),\n",
    "            np.concatenate([self.y_fit_true, self.y_predict_true]),\n",
    "            test_size=0.9)\n",
    "\n",
    "        return svm.SVC(gamma='scale', kernel='rbf').fit(self.x_fit, self.y_fit_true)\n",
    "\n",
    "\n",
    "class WeightsAnalyzer(Analyzer):\n",
    "    def __init__(self, fp: FingerPrint, rd: ResponseData):\n",
    "        super().__init__(fp, rd)\n",
    "\n",
    "        self._init_data()\n",
    "        self.clf = self._classify()\n",
    "        self.predictions = self.clf.predict(self.x_predict)\n",
    "\n",
    "        self.score = cross_val_score(self.clf, self.x_fit, y=self.y_fit_true, cv=5, scoring='f1_weighted')\n",
    "\n",
    "\n",
    "    def _score_sessions(self, sessions: list, label: int):\n",
    "        _ = []\n",
    "        __ = []\n",
    "\n",
    "        for session in sessions:\n",
    "            session_score = 0\n",
    "\n",
    "            for feature in self.fp.weights.keys():\n",
    "                if feature in session:\n",
    "                    session_score += 1\n",
    "\n",
    "            _.append([session_score])\n",
    "            __.append(label)\n",
    "\n",
    "        return np.array(_), np.array(__)\n",
    "\n",
    "\n",
    "    def _init_data(self):\n",
    "        self.x_fit, self.y_fit_true = self._score_sessions(self.fp.sessions, 1)\n",
    "        self.x_predict, self.y_predict_true = self._score_sessions(self.rd.sessions, 0)\n",
    "\n",
    "\n",
    "    def _classify(self):\n",
    "        self.x_fit, self.x_predict, self.y_fit_true, self.y_predict_true = train_test_split(\n",
    "            np.concatenate([self.x_fit, self.x_predict]), np.concatenate([self.y_fit_true, self.y_predict_true]),\n",
    "            test_size=0.9)\n",
    "\n",
    "        return svm.SVC(gamma='scale', kernel='rbf').fit(self.x_fit, self.y_fit_true)\n",
    "\n",
    "\n",
    "class SumsAnalyzer(Analyzer):\n",
    "    def __init__(self, fp: FingerPrint, rd: ResponseData):\n",
    "        super().__init__(fp, rd)\n",
    "\n",
    "        self._init_data()\n",
    "        self.clf = self._classify()\n",
    "        self.predictions = self.clf.predict(self.x_predict)\n",
    "\n",
    "        self.score = cross_val_score(self.clf, self.x_fit, y=self.y_fit_true, cv=5, scoring='f1_weighted')\n",
    "\n",
    "\n",
    "    def _init_data(self):\n",
    "        length = min(len(self.fp.hars), len(self.rd.hars))\n",
    "\n",
    "        self.x_fit, self.x_predict, self.y_fit_true, self.y_predict_true = train_test_split(\n",
    "            np.concatenate([self.fp.sums[:length], self.rd.sums[:length]]), [1] * length + [0] * length, test_size=0.9)\n",
    "\n",
    "\n",
    "    def _classify(self):\n",
    "        return svm.SVC(gamma='scale', kernel='rbf').fit(self.x_fit, self.y_fit_true)\n",
    "\n",
    "\n",
    "    def plot_scatter(self):\n",
    "        x_fp = np.array(self.fp.sums)[:, 0]\n",
    "        y_fp = np.array(self.fp.sums)[:, 1]\n",
    "\n",
    "        x_rd = np.array(self.rd.sums)[:, 0]\n",
    "        y_rd = np.array(self.rd.sums)[:, 1]\n",
    "\n",
    "        plt.scatter(x_rd, y_rd, label='other')\n",
    "        plt.scatter(x_fp, y_fp, label='facebook')\n",
    "        plt.legend()\n",
    "        plt.ylabel('Size')\n",
    "        plt.xlabel('Time')\n",
    "\n",
    "\n",
    "class CombinedAnalyzer(Analyzer):\n",
    "    def __init__(self, fp: FingerPrint, rd: ResponseData):\n",
    "        super().__init__(fp, rd)\n",
    "\n",
    "        self.dict_vectorizer = DictVectorizer(sparse=False)\n",
    "        self._init_data()\n",
    "        self.clf = self._classify()\n",
    "        self.predictions = self.clf.predict(self.x_predict)\n",
    "\n",
    "        self.score = cross_val_score(self.clf, self.x_fit, y=self.y_fit_true, cv=5, scoring='f1_weighted')\n",
    "\n",
    "\n",
    "    def _init_data(self):\n",
    "        length = min(self.fp.length, self.rd.length)\n",
    "\n",
    "        # Add the type feature.\n",
    "        self.x_fit = self.dict_vectorizer.fit_transform(self.fp.types_counts[:length]).tolist()\n",
    "        self.x_predict = self.dict_vectorizer.transform(self.rd.types_counts[:length]).tolist()\n",
    "\n",
    "        for session_i in range(length):\n",
    "            # Add the sums feature.\n",
    "            self._add_sums(session_i)\n",
    "\n",
    "            # Add the weight score. 0 by default.\n",
    "            self.x_fit[session_i].append(0)\n",
    "            self.x_predict[session_i].append(0)\n",
    "\n",
    "            # Add the weights feature.\n",
    "            self._add_weights(session_i)\n",
    "\n",
    "            # Add labels\n",
    "            self.y_fit_true.append(1)\n",
    "            self.y_predict_true.append(0)\n",
    "\n",
    "\n",
    "    def _add_sums(self, i):\n",
    "        self.x_fit[i].append(self.fp.sums[i][0])\n",
    "        self.x_fit[i].append(self.fp.sums[i][1])\n",
    "        self.x_predict[i].append(self.rd.sums[i][0])\n",
    "        self.x_predict[i].append(self.rd.sums[i][1])\n",
    "\n",
    "\n",
    "    def _add_weights(self, i):\n",
    "        for feature in self.fp.weights.keys():\n",
    "            if feature in self.fp.sessions[i]:\n",
    "                self.x_fit[i][-1] += 1\n",
    "            if feature in self.rd.sessions[i]:\n",
    "                self.x_predict[i][-1] += 1\n",
    "\n",
    "\n",
    "    def _classify(self):\n",
    "        self.x_fit, self.x_predict, self.y_fit_true, self.y_predict_true = train_test_split(\n",
    "            np.concatenate([self.x_fit, self.x_predict]), np.concatenate([self.y_fit_true, self.y_predict_true]),\n",
    "            test_size=0.9)\n",
    "\n",
    "        return svm.SVC(gamma='scale', kernel='rbf').fit(self.x_fit, self.y_fit_true)\n",
    "\n",
    "\n",
    "class UsersAnalyzer:\n",
    "    def __init__(self, data: list, flags: str = 't'):\n",
    "        self.flags = flags\n",
    "        self.data = data\n",
    "        self.dict_vectorizer = DictVectorizer(sparse=False)\n",
    "        self.x_fit = []\n",
    "        self.y_fit_true = []\n",
    "\n",
    "        self._init()\n",
    "        self.clf = self._classify()\n",
    "        self.predictions = self.clf.predict(self.x_predict)\n",
    "        self.score = cross_val_score(self.clf, self.x_fit, y=self.y_fit_true, cv=5)\n",
    "\n",
    "\n",
    "    def _init(self):\n",
    "        if 't' in self.flags:\n",
    "            _ = reduce(lambda a, b: a + b, [x.types_counts for x in self.data])\n",
    "            self.x_fit = self.dict_vectorizer.fit_transform(_)\n",
    "        elif 's' in self.flags:\n",
    "            self.x_fit = reduce(lambda a, b: a + b, [x.sums for x in self.data])\n",
    "        elif 'w' in self.flags:\n",
    "            self.x_fit = reduce(lambda a, b: a + b, [x.weights for x in self.data])\n",
    "\n",
    "        self.y_fit_true = reduce(lambda a, b: a + b,\n",
    "                                 [[i] * len(self.data[i].types_counts) for i in range(len(self.data))])\n",
    "\n",
    "\n",
    "    def _classify(self):\n",
    "        self.x_fit, self.x_predict, self.y_fit_true, self.y_predict_true = train_test_split(self.x_fit, self.y_fit_true,\n",
    "                                                                                            test_size=0.4)\n",
    "\n",
    "        return svm.SVC(gamma='scale', kernel='rbf').fit(self.x_fit, self.y_fit_true)\n",
    "\n",
    "\n",
    "def page_func(driver, page):\n",
    "    \"\"\"Passed to the Har.capture_n_har_files procedure for selenium to run\n",
    "    on the web page.\n",
    "    \"\"\"\n",
    "    timeout = 10\n",
    "\n",
    "    email_xpath = '//input[@id=\"email\"] | //input[@name=\"email\"]'\n",
    "    pass_xpath = '//input[@id=\"pass\"] | //input[@name=\"pass\"]'\n",
    "    login_xpath = '//input[@value=\"Log In\"] | //button[@name=\"login\"]'\n",
    "    page_xpath = '//span[text()=\"' + page + '\"]'\n",
    "\n",
    "    # Make sure all elements exist on page before moving on.\n",
    "    run = True\n",
    "\n",
    "    while run:\n",
    "        try:\n",
    "            WebDriverWait(driver, timeout).until(EC.presence_of_element_located((By.XPATH, email_xpath)))\n",
    "            WebDriverWait(driver, timeout).until(EC.presence_of_element_located((By.XPATH, login_xpath)))\n",
    "            WebDriverWait(driver, timeout).until(EC.presence_of_element_located((By.XPATH, pass_xpath)))\n",
    "\n",
    "            run = False\n",
    "        except selenium.common.exceptions.NoSuchElementException:\n",
    "            print('Login NoSuchElementException.')\n",
    "        except selenium.common.exceptions.TimeoutException:\n",
    "            print('Login TimeoutException.')\n",
    "\n",
    "    driver.find_element_by_xpath(email_xpath).send_keys('gggppp282@gmail.com')\n",
    "    driver.find_element_by_xpath(pass_xpath).send_keys('g31012310G')\n",
    "    driver.find_element_by_xpath(login_xpath).click()\n",
    "\n",
    "    # Make sure all elements exist on page before moving on.\n",
    "    run = True\n",
    "\n",
    "    while run:\n",
    "        try:\n",
    "            WebDriverWait(driver, timeout).until(EC.presence_of_element_located((By.XPATH, '//div[text()=\"Pages\"]')))\n",
    "\n",
    "            run = False\n",
    "        except selenium.common.exceptions.NoSuchElementException:\n",
    "            print('Login NoSuchElementException.')\n",
    "        except selenium.common.exceptions.TimeoutException:\n",
    "            print('Login TimeoutException.')\n",
    "\n",
    "    driver.find_element_by_xpath('//div[text()=\"Pages\"]').click()\n",
    "\n",
    "    # Make sure all elements exist on page before moving on.\n",
    "    run = True\n",
    "\n",
    "    while run:\n",
    "        try:\n",
    "            WebDriverWait(driver, timeout).until(EC.presence_of_element_located((By.XPATH, page_xpath)))\n",
    "\n",
    "            run = False\n",
    "        except selenium.common.exceptions.NoSuchElementException:\n",
    "            print('Login NoSuchElementException.')\n",
    "        except selenium.common.exceptions.TimeoutException:\n",
    "            print('Login TimeoutException.')\n",
    "\n",
    "    driver.find_element_by_xpath(page_xpath).click()\n",
    "\n",
    "\n",
    "def capture_har_data(n, page_func=None, page=None):\n",
    "    \"\"\"Utility procedure to create n HAR files, both for the fingerprint\n",
    "        and random.\n",
    "    \"\"\"\n",
    "    for i in range(n):\n",
    "        print('\\nCapturing FingerPrint HAR data...')\n",
    "        Har.capture_n_har_files(path='./har_fit', n=1, url='https://www.facebook.com', name='facebook',\n",
    "                                page_func=page_func, page=page)\n",
    "\n",
    "        # print('\\nCapturing ResponseData HAR data...')\n",
    "        # Har.capture_n_har_files(path='./har_random', n=1, rnd=True)\n",
    "\n",
    "\n",
    "def run_analyzers(fp, rd):\n",
    "    wa = WeightsAnalyzer(fp, rd)\n",
    "    sa = SumsAnalyzer(fp, rd)\n",
    "    ta = TypesAnalyzer(fp, rd)\n",
    "    ca = CombinedAnalyzer(fp, rd)\n",
    "\n",
    "    wa.plot_confusion_matrix(title='WeightsAnalyzer')\n",
    "    sa.plot_confusion_matrix(title='SumsAnalyzer')\n",
    "    ta.plot_confusion_matrix(title='TypesAnalyzer')\n",
    "    ca.plot_confusion_matrix(title='CombinedAnalyzer')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
